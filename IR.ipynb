{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One',\n",
       " 'evening',\n",
       " 'Frodo',\n",
       " 'and',\n",
       " 'Sam',\n",
       " 'were',\n",
       " 'walking',\n",
       " 'together',\n",
       " 'in',\n",
       " 'the',\n",
       " 'cool',\n",
       " 'twilight']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "sms = ('One evening Frodo and Sam were walking together in the cool twilight')\n",
    "\n",
    "tokenized_sms = word_tokenize(sms)\n",
    "tokenized_sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['evening', 'and', 'were', 'together', 'the', 'twilight']]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stopwor_sms=[]\n",
    "stop_sm=stopword_sms \n",
    "stop_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "even\n",
      "frodo\n",
      "and\n",
      "sam\n",
      "were\n",
      "walk\n",
      "togeth\n",
      "in\n",
      "the\n",
      "cool\n",
      "twilight\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemm = (tokenized_sms)\n",
    "for stemm in stemm :\n",
    "    stemm = stemmer.stem(stemm)\n",
    "    print(stemmer.stem(stemm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "evening\n",
      "Frodo\n",
      "and\n",
      "Sam\n",
      "were\n",
      "walking\n",
      "together\n",
      "in\n",
      "the\n",
      "cool\n",
      "twilight\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import *\n",
    "stemmer = WordNetLemmatizer()\n",
    "plurals = tokenized_sms\n",
    "for plurals in plurals :\n",
    "    print(stemmer.lemmatize(plurals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# خروجی استریم را وارد لیست می کنیم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'even',\n",
       " 'frodo',\n",
       " 'and',\n",
       " 'sam',\n",
       " 'were',\n",
       " 'walk',\n",
       " 'togeth',\n",
       " 'in',\n",
       " 'the',\n",
       " 'cool',\n",
       " 'twilight']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for i in range(len(tokenized_sms)):\n",
    "    tokenized_sms[i] = stemmer.stem(tokenized_sms[i])\n",
    "    \n",
    "tokenized_sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autocorrect import spell\n",
    "# tokenized_sms[i] = stemmer.stem(spell(tokenized_sms[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sms_text = \" \".join(tokenized_sms)\n",
    "# sms_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# matrix = CountVectorizer(max_features=12)\n",
    "# X = matrix.fit_transform(tokenized_sms).toarray()\n",
    "# X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0]]\n",
      "{'one': 5, 'even': 2, 'frodo': 3, 'and': 0, 'sam': 6, 'were': 11, 'walk': 10, 'togeth': 8, 'in': 4, 'the': 7, 'cool': 1, 'twilight': 9}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'one': 5,\n",
       " 'even': 2,\n",
       " 'frodo': 3,\n",
       " 'and': 0,\n",
       " 'sam': 6,\n",
       " 'were': 11,\n",
       " 'walk': 10,\n",
       " 'togeth': 8,\n",
       " 'in': 4,\n",
       " 'the': 7,\n",
       " 'cool': 1,\n",
       " 'twilight': 9}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  # Feature extraction from text\n",
    "# Method: bag of words \n",
    "# https://pythonprogramminglanguage.com\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "print( vectorizer.fit_transform(tokenized_sms).todense() )\n",
    "print( vectorizer.vocabulary_ )\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frodo', 'sam', 'walk', 'togeth', 'cool', 'twilight']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = []\n",
    "for tokenized_sms in tokenized_sms:\n",
    "     for word,pos in nltk.pos_tag(nltk.word_tokenize(str(tokenized_sms))):\n",
    "            if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                nouns.append(word)\n",
    "nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boolean retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups                # DOWNLOAD DATA SET\n",
    "twenty = fetch_20newsgroups()\n",
    "\n",
    "tfidf = TfidfVectorizer().fit_transform(twenty.data)         #   همه لغات tfidf محاسبه \n",
    "\n",
    "print (twenty.data[0]) # تست \n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 56979)\t0.0574701540748513\n",
      "  (0, 75358)\t0.3538350134970617\n",
      "  (0, 123162)\t0.25970902457356887\n",
      "  (0, 118280)\t0.21186807208281694\n",
      "  (0, 50527)\t0.05461428658858725\n",
      "  (0, 124031)\t0.10798795154169123\n",
      "  (0, 85354)\t0.03696978508816317\n",
      "  (0, 114688)\t0.06214070986309587\n",
      "  (0, 111322)\t0.019156718024950434\n",
      "  (0, 123984)\t0.036854292634593756\n",
      "  (0, 37780)\t0.3813389125949312\n",
      "  (0, 68532)\t0.07325812342131598\n",
      "  (0, 114731)\t0.1444727551278406\n",
      "  (0, 87620)\t0.0356718631408158\n",
      "  (0, 95162)\t0.03447138409326312\n",
      "  (0, 64095)\t0.035420924271313554\n",
      "  (0, 98949)\t0.16068606055394935\n",
      "  (0, 90379)\t0.01992885995664587\n",
      "  (0, 118983)\t0.03708597805061915\n",
      "  (0, 89362)\t0.06521174306303765\n",
      "  (0, 79666)\t0.10936401252414275\n",
      "  (0, 40998)\t0.07801368196918111\n",
      "  (0, 92081)\t0.09913274493911224\n",
      "  (0, 76032)\t0.01921946305222309\n",
      "  (0, 4605)\t0.06332603952480324\n",
      "  :\t:\n",
      "  (0, 37565)\t0.03431760442478462\n",
      "  (0, 113986)\t0.17691750674853085\n",
      "  (0, 83256)\t0.08844382496462175\n",
      "  (0, 86001)\t0.07000411445838192\n",
      "  (0, 51730)\t0.09714744057976724\n",
      "  (0, 109271)\t0.10844724822064675\n",
      "  (0, 128026)\t0.06062209588975889\n",
      "  (0, 96144)\t0.10826904490745742\n",
      "  (0, 78784)\t0.0633940918806495\n",
      "  (0, 63363)\t0.08342748387969037\n",
      "  (0, 90252)\t0.03188936879541757\n",
      "  (0, 123989)\t0.08207027465330355\n",
      "  (0, 67156)\t0.0731344392274018\n",
      "  (0, 128402)\t0.059222940832778424\n",
      "  (0, 62221)\t0.029215279924278678\n",
      "  (0, 57308)\t0.15587170091577043\n",
      "  (0, 76722)\t0.06908779999621749\n",
      "  (0, 94362)\t0.05545703139014723\n",
      "  (0, 78955)\t0.059898568880615996\n",
      "  (0, 114428)\t0.05511105154696677\n",
      "  (0, 66098)\t0.09785515708314482\n",
      "  (0, 35187)\t0.09353930598317126\n",
      "  (0, 35983)\t0.037704485636198756\n",
      "  (0, 128420)\t0.042784990792830935\n",
      "  (0, 86580)\t0.1315711871424099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x130107 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 89 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[0:1]   # انتخاب مقدار مشخصی از tfidf\n",
    "print(tfidf[0:1])\n",
    "tfidf[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.04405974, 0.11017033, ..., 0.04433678, 0.04457107,\n",
       "       0.0329325 ])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,   958, 10576,  3277], dtype=int64)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "related_docs_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.54967949, 0.32902198, 0.28257872])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities[related_docs_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
